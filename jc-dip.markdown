---
title: Journal Club of  the Department of Image Processing
layout: splash

---
# Journal Club of  the Department of Image Processing 
Welcome to the Journal Club of the Department of Image Processing. Our group meets regularly on Fridays in every 2-3 weeks. Each session is hosted by one of the members and includes an article presentation and reading it thoroughly. The presentation is followed by a discussion.

If you're interested check out the [rules](#rules) below. 

## Talks
 <table>
  <tr>
    <th>Date</th>
    <th>Moderator</th>
    <th>Topic</th>
    <th>To Read</th>
    <th>Slides</th>
  </tr>
  <tr>
    <td>02.12.2022</td>
    <td>T. Ka. </td>
    <td>A Survey of Visual Transformers</td>
    <td> <a href="https://arxiv.org/abs/2111.06091"><img src="https://shields.io/static/v1?label=arXiv&message=2111.06091&color=orange&?style=plastic" alt="Arxiv shield"></a></td>
    <td> <a href="https://docs.google.com/presentation/d/1_T1et-binyg8qsAHci2Ra9ltgMNt8AlVrrYosMsUNM0/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>20.01.2023</td>
    <td>T. Ka. </td>
    <td>MaxViT: Multi-Axis Vision Transformer </td>
    <td> <a href="https://arxiv.org/abs/2204.01697"><img src="https://shields.io/static/v1?label=arXiv&message=2204.01697&color=orange&?style=plastic"></a></td>
    <td> <a href="https://docs.google.com/presentation/d/1fdJ1NeP4aAShbfbbdaQ7pna38QlkkHO0a1KqrnNBkdM/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>14.02.2023</td>
    <td>A. Ha. </td>
    <td>ConvNeXt: A ConvNet for the 2020s </td>
    <td><a href="https://arxiv.org/abs/2201.03545"><img src="https://shields.io/static/v1?label=arXiv&message=2201.03545&color=orange&?style=plastic"></a></td>
    <td><a href="https://docs.google.com/presentation/d/18NlW37TkP6UkAuHi94HgfVg5YfD4B7C06qyzGAdAbaM/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>03.03.2023</td>
    <td>T. Ke. </td>
    <td>DMs: Denoising Diffusion Probabilistic Models</td>
    <td><a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html"><img src="https://shields.io/static/v1?label=NeurIPS&message=33 2020&color=blue&?style=plastic"></a></td>
    <td><a href="https://docs.google.com/presentation/d/1JruvQVGqtmOJZVVznD9BRDTr3VykVig1/edit?usp=sharing&ouid=107985672988319725914&rtpof=true&sd=true"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>24.03.2023</td>
    <td>T. Ka. </td>
    <td>LDMs: High-Resolution Image Synthesis with Latent Diffusion Models </td>
    <td><a href="https://arxiv.org/abs/2112.10752"><img src="https://shields.io/static/v1?label=arXiv&message=2112.10752&color=orange&?style=plastic"></a></td>
    <td><a href="https://docs.google.com/presentation/d/1tHtirGWaJnrQfXMaWafv3xGxesQ9nB49GgEZ9GvsKkU/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>14.04.2023</td>
    <td>T. Ka. </td>
    <td>How Do Vision Transformers Work?</td>
    <td><a href="https://arxiv.org/abs/2202.06709"><img src="https://shields.io/static/v1?label=arXiv&message=2202.06709&color=orange&?style=plastic"></a></td>
    <td><a href="https://docs.google.com/presentation/d/1TjvukOqeOE029nc5eylDU53D7guFQaMKG6aIysjD31g/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
  </tr>
  <tr>
    <td>05.05.2023</td>
    <td>T. Ka. </td>
    <td>SAM: Segment Anything</td>
    <td><a href="https://arxiv.org/abs/2304.02643"><img src="https://shields.io/static/v1?label=arXiv&message=2304.02643&color=orange&?style=plastic"></a></td>
    <td><a href="https://docs.google.com/presentation/d/1X8KK8I2-X5fcWCHUIGSUaVArN7gG_uIET5JsqvV_sCA/edit?usp=sharing"><img src="https://shields.io/static/v1?label=Slides&message=Link&?style=plastic&logo=google"></a></td>
</tr>
<tr style="background-color: slategray">
    <td>19.05.2023</td>
    <td>A. No.</td>
    <td>CLIP: Learning Transferable Visual Models From Natural Language Supervision</td>
    <td><a href=" https://arxiv.org/pdf/2103.00020"><img src="https://shields.io/static/v1?label=arXiv&message=2103.00020&color=orange&?style=plastic"></a></td>
    <td> </td>
</tr><tr style="background-color: slategray">
    <td>02.06.2023</td>
    <td>A. Ha. </td>
    <td>DINO: Emerging Properties in Self-Supervised Vision Transformers</td>
    <td><a href="https://arxiv.org/pdf/2104.14294.pdf"><img src="https://shields.io/static/v1?label=arXiv&message=2104.14294&color=orange&?style=plastic"></a></td>
    <td> </td>

</tr>

</table> 

## Wishlist 
<table> 
<tr>
    <td>DINOv2 </td>
    <td><a href=" https://arxiv.org/abs/2304.07193"><img src="https://shields.io/static/v1?label=DINOv2&message=2304.07193&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>NERF </td>
    <td><a href=" https://arxiv.org/abs/2003.08934"><img src="https://shields.io/static/v1?label=NERF&message=2003.08934&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>GLIDE </td>
    <td><a href=" https://arxiv.org/abs/2112.10741"><img src="https://shields.io/static/v1?label=GLIDE&message=2112.10741&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>Matryoshka Representation Learning </td>
    <td><a href=" https://arxiv.org/abs/2205.13147"><img src="https://shields.io/static/v1?label=Matryoshka&message=2205.13147&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>NVAE </td>
    <td><a href=" https://proceedings.neurips.cc/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf"><img src="https://shields.io/static/v1?label=NeurIPS&message=2020&color=blue&?style=plastic"></a></td>
</tr><tr>
    <td>Deep Unsupervised Learning using Nonequilibrium Thermodynamics </td>
    <td><a href=" https://arxiv.org/abs/1503.03585"><img src="https://shields.io/static/v1?label=DF&message=1503.03585&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>Forward Forward Alg </td>
    <td><a href=" https://arxiv.org/abs/2212.13345"><img src="https://shields.io/static/v1?label=F-F alg&message=2212.13345&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>Choose Your Weapon: Survival Strategies for Depressed AI Academics </td>
    <td><a href=" https://arxiv.org/abs/2304.06035"><img src="https://shields.io/static/v1?label=Choose &message=2304.06035&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>Segment Everything Everywhere All at Once </td>
    <td><a href=" https://arxiv.org/abs/2304.06718"><img src="https://shields.io/static/v1?label=SEEM&message=2304.06718&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr><tr>
    <td>LORA: Low-Rank Adaptation of Large Language Models</td>
    <td><a href="https://arxiv.org/pdf/2106.09685.pdf"><img src="https://shields.io/static/v1?label=LORA&message=2106.09685&color=orange&?style=plastic&logo=arxiv"></a></td>
</tr>
</table>

## Before bedtime reading
<table>
<tr>
    <td> Attention is all you need (Vaswani 2017) </td> 
    <td><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"><img src="https://shields.io/static/v1?label=NeurIPS&message=2017&color=blue&?style=plastic"></a></td>
</tr>
<tr>
    <td>BERT: Pre-training of Deep Bidirectional Transformers </td>
    <td><a href="https://arxiv.org/abs/1810.04805"><img src="https://shields.io/static/v1?label=arXiv&message=1810.04805&color=orange&?style=plastic"></a></td>
</tr>
<tr>
<td>ViT: An Image is Worth 16x16 Words (Dosovitskiy et al. 2021)</td>
<td><a href="https://arxiv.org/abs/2010.11929"><img src="https://shields.io/static/v1?label=arXiv&message=2010.11929&color=orange&?style=plastic"></a></td>
</tr>
</table>

## Rules

If you would like to join our club there are three rules to be followed:

1. Every member has to read an article to be presented in advance.
2. Every member has to propose a topic that she/he is willing to present as a moderator.
3. Every member can give a preferential vote to published topics.


